
Autotest
--------

 * Rebase to a newer upstream autotest version.

 * xfstests
     * Expand the xfstest to cover more than ext4. We were doing all
       filesystems we cared about but were running into a lot of
       failures.

     * Look at the failures and see which of them are failures that
       we need to be doing something about and start working them.

 * We should probably be rebooting after each individual autotest test
   is run. We may want to completely re-provision the system. This is
   something that will be difficult to "just slip in" for QA.

 * Thinking about pulling the qrt tests into separate tests by having
   'qrt-apparmor', 'qrt-aslr', etc.

 * I'm thinking it might be nice to run the xfstests per filesystem
   than one test which exercises all filesystems. Kind of a 'xfstests-ext4',
   'xfstests-xfs', etc.

kernel-testing
--------------

  * We need to be getting _some_ kind of results when tests fail.

  * Improve testing robustness:
    * Far too often I am finding jenkins jobs "stuck" on a particular
      test for hours at a time. We need some way of timing out these
      tests and still producing results.

    * Again, I am repeatedly finding tests have failed for some unknown
      reason and what I find in the console log is a java call-stack
      trace. We end up with *no* results after hours of testing and
      we don't even know the test has failed except that I know to go
      looking for the results of a job that I started by hand. If this
      was automated, I wouldn't even know to go look.

  * Fix HWE kernel testing for non-quantal kernels.

  * Fix virtual testing. Most of the necessary infrastructure is
    in place, it just needs to be wrung out.

  * Add support for CEPH testing. This is going to require some
    custom provisioning but just builds on what we already have.

  * Need a tool/process which prunes the metrics/test-results.

Re-work?
--------

  * Automatically determining which autotest control file to use
    was not the best nor simpleest idea. I think we would be
    better off using the "testDependency" dictionary to explicitly
    set which control file to use.

    If we are thinking of that, we should think about something
    else that Steve has mentioned which is to have the control
    files outside the autotest tree and insert them into the
    tree based on the series. I can see where we could expand
    this idea to other files as well. For example if the lucid
    version of xfstests wanted additional, ubuntu specific
    patches we could keep those out-of-tree as well and copy
    them in-tree when we are on a lucid system.

  * jjc is a nice idea but I question the implementation. Mostly
    it's the use of the macro files and how complex they have
    gotten. Could this be simplified using simpler macro files
    which execute shell scripts?

  * The autogenerated script files which then get "sourced" by
    the kernel-tests-runner script seem like a hack (though I
    don't have a better solution)

  * The entire multiple, jenkins jobs process is a mess. I'd like
    to see this changed considerably.

    - A single, simple script that gets copied to the SUT via
      scp and then executed via ssh.

      1. No dependencies on Jenkins environment varieables.
      2. No post processing of test results or copying the
         results back to the server. The server can pull these
         results back if desired.

    - A single jenkins job that does the provisioning, pushes
      the test script to the sut and collects the results.
