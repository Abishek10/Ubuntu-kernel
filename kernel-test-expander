#!/usr/bin/env python
#
# Process a list of requested tests and testcollections
# Create shell scripts to be used to set them up and run them
#

from os                                 import getenv, listdir, path
from sys                                import exit, argv
from subprocess                         import Popen, PIPE
from lib.utils                          import run_command
import json

# Test dependencies are organized by autotest test name. They include:
#
# 'package': required packages which will be installed before running tests
# 'command': A command line which will be executed before running tests
# 'scratch': If this exists, it indicates a requirement for a scratch device,
#            and triggers partitioning and formatting of that device
# 'atargs' : Autotest arguments to be passed to the control file. Can include expansion of shell variables
#
testDependencies = {
    'ubuntu_qrt_kernel' : {
        'packages' : {
            'common' : [
                'build-essential', 'libcap2-bin', 'gcc-multilib', 'gdb', 'gawk', 'execstack', 'exim4', 'libcap-dev', 'gdb',
                ],
            },
        },

    'ubuntu_qrt_kernel_aslr_collisions' : {
        'packages' : {
            'common' : [
                'build-essential', 'libcap2-bin', 'gcc-multilib', 'gdb', 'gawk', 'execstack', 'exim4', 'libcap-dev', 'gdb',
                ],
            },
        },

    'ubuntu_qrt_kernel_hardening' : {
        'packages' : {
            'common' : [
                'build-essential', 'libcap2-bin', 'gcc-multilib', 'gdb', 'gawk', 'execstack', 'exim4', 'libcap-dev', 'gdb',
                ],
            },
        },

    'ubuntu_qrt_kernel_panic' : {
        'packages' : {
            'common' : [
                'build-essential', 'libcap2-bin', 'gcc-multilib', 'gdb', 'gawk', 'execstack', 'exim4', 'libcap-dev', 'gdb',
                ],
            },
        },

    'ubuntu_qrt_kernel_security' : {
        'packages' : {
            'common' : [
                'build-essential', 'libcap2-bin', 'gcc-multilib', 'gdb', 'gawk', 'execstack', 'exim4', 'libcap-dev', 'gdb',
                ],
            },
        },

    'ubuntu_qrt_apparmor' : {
        'packages' : {
            'common' : [
                'build-essential', 'libcap2-bin', 'gcc-multilib', 'gdb', 'gawk', 'execstack', 'exim4', 'libcap-dev', 'gdb',
                'python-pexpect', 'apparmor', 'apparmor-utils', 'netcat', 'sudo', 'build-essential', 'libapparmor-dev',
                'ruby1.8', 'attr', 'apport', 'libpam-apparmor', 'libgtk2.0-dev', 'pyflakes', 'apparmor-profiles', 'quilt',
                'libdbus-1-dev'
                ],
            'trusty' : [
                'python3-libapparmor', 'python-libapparmor', 'python3', 'python3-all-dev',
                ],
            'saucy' : [
                'python3-libapparmor', 'python-libapparmor', 'python3', 'python3-all-dev',
                ],
            'raring' : [
                'python3-libapparmor', 'python-libapparmor', 'python3', 'python3-all-dev',
                ],
            'quantal' : [
                'python3-libapparmor', 'python-libapparmor', 'python3', 'python3-all-dev',
                ],
            'precise' : [
                'python-libapparmor', 'python3', 'python3-all-dev',
                ],
            },
        },

    'ubuntu_ecryptfs' : {
        'packages' : {
            'common' : [
                'libglib2.0-dev', 'intltool', 'keyutils', 'libkeyutils-dev', 'libpam0g-dev', 'libnss3-dev', 'libtool', 'acl', 'xfsprogs', 'btrfs-tools', 'gdb',
                ],
            },
        },

    'ubuntu_leap_seconds' : {
        'packages' : {
            'common' : [],
            },
        },

    'xfstests' : {
        'packages' : {
            'common' : [
                'uuid-dev', 'xfslibs-dev', 'xfsdump', 'autoconf', 'kpartx', 'libtool', 'python-xattr', 'libacl1-dev', 'libaio-dev', 'quota', 'bc', 'libdm0-dev', 'btrfs-tools', 'attr', 'gdb',
                ],
            },
        'command' : 'sudo mkdir -p /media/xfsmount ; sudo adduser --quiet --disabled-password -gecos "XFS test user,,," fsgqa || true',
        'atargs' : {}, # this gets filled in later, it's a hack
        'scratch' : True,
        },

    'iperf' : {
        'packages' : {
            'common' : ['sysstat', 'gdb'],
            },
        },

    'ltp' : {
        'packages' : {
            'common' : ['flex'],
            },
        },

    'power_consumption' : {
        'packages' : {
           'common' : ['stress', 'chromium-browser'],
            },
        'atargs' : {'METER_ADDR':'10.97.8.2', 'METER_PORT':'3490', 'METER_TAGPORT':'9999'},
        },

    'wakeup_events' : {
        'packages' : {
            'common' : [],
            },
        },

}

# Test Collections may be defined, and will run the list of autotest tests in them
#
testCollections = {
    'benchmarks' : ['dbench', 'compilebench', 'bonnie'],

    # This is the set of tests that will run if no KERNEL_TEST_LIST variable is set.
    # (QA runs this set)
    #
    'default' : ['ubuntu_ecryptfs', 'ubuntu_qrt_kernel_hardening', 'ubuntu_qrt_kernel_panic', 'ubuntu_qrt_kernel_security', 'ubuntu_qrt_kernel_aslr_collisions', 'ubuntu_qrt_apparmor'],

    # This is the set of tests that the kernel team runs. This should
    # be a superset of the 'default' tests that QA runs.
    #
    'kernel' : ['iperf', 'ubuntu_leap_seconds', 'stress', 'ltp', 'xfstests'],

    # This is the set of tests for measuring power consumption.
    'power' : ['power_consumption'],

    # This is the set of tests for measuring power consumption.
    'wakeup_events' : ['wakeup_events'],
}

# Exit
#
class Exit():
    """
    If an error message has already been displayed and we want to just exit the app, this
    exception is raised.
    """
    pass

# ExpandTests
#
class ExpandTests():
    """
    """

    def __init__(self):
        self.__releaseName = None

    def mounts(self):
        p = Popen(['mount'], stdout=PIPE, stderr=PIPE, close_fds=True)
        retval = p.communicate()[0].split('\n')
        return retval

    def root_dev(self):
        retval='bogus'
        for line in self.mounts():
            if ' / ' in line:
                retval = line.split()[0].replace('/dev/', '')[0:2]
        return retval

    def kind_of_hardware(self):
        retval = 'real'
        with open('/proc/cpuinfo', 'r') as f:
            line = f.readline()
            while line:
                if 'model name' in line:
                    if 'QEMU Virtual CPU' in line:
                        retval = 'virtual'
                    break
                line = f.readline()
        return retval

    # releaseName
    #
    @property
    def releaseName(self):
        if self.__releaseName is None:
            status, info = run_command("lsb_release -a")
            for line in info:
                parts = line.split(":")
                if parts[0] == "Codename":
                    self.__releaseName = parts[1].strip()
                    print("Release Name is %s" % self.__releaseName)
                    break
        return self.__releaseName

    # merge_test_options
    #
    def merge_test_options(self):
        '''
        The environment variable KERNEL_TEST_OPTIONS can be used to 'modify'
        the information used for the various tests. It can be used to specify
        arguments to be passed on the autotest command line on a per-test
        basis.

        For example, to pass extra arguments to the xfstests via the command
        line you can set the variable to "{'xfstests':{'atargs':{'FILE_SYSTEMS':'xfs'}}}".

        The KERNEL_TEST_OPTIONS string should be a valid json format string.
        '''
        kto = getenv('KERNEL_TEST_OPTIONS')
        if kto:
            mods = json.loads(kto)
            for k in mods:
                for m in mods[k]:
                    testDependencies[k][m] = mods[k][m]

    # main
    #
    def main(self):
        retval = 0
        scratchNeeded = False
        envVars = {}

        try:
            self.merge_test_options()

            # Which tests are we supposed to run?
            tlist =  getenv('KERNEL_TEST_LIST')
            if tlist:
                print('%s: Tests requested: %s' % (argv[0], tlist))
            else:
                tlist = 'default'

            # Now expand this list to a unique list of autotest test names
            autotestToRun = []
            for tname in tlist.split():
                # expand any test collection lists
                if tname in testCollections:
                    print('%s: Expanding Test collection %s to the following Autotest Tests:' % (argv[0], tname))
                    for atst in testCollections[tname]:
                        print('    %s' % atst)
                        if atst not in autotestToRun:
                            autotestToRun.append(atst)
                else:
                    # Simply add the test to the list
                    print('%s: Adding Autotest Test %s' % (tname, argv[0]))
                    if tname not in autotestToRun:
                        autotestToRun.append(tname)

            # Find out which autotest tests are available, to use for sanity test
            autotest_tests_path = 'autotest/client/tests'
            if not path.exists(autotest_tests_path):
                print('%s: *** Error: The autotest client tests were not found (%s).' % (argv[0], autotest_tests_path))
                raise Exit()

            autotestExisting = listdir('autotest/client/tests')

            # Make a sanity check against what's available, and set flag if we need scratch
            badtests = []
            for tname in autotestToRun:
                if tname in testDependencies:
                    if 'scratch' in testDependencies[tname]:
                        # Scratch device required
                        scratchNeeded = True
                if tname not in autotestExisting:
                    badtests.append(tname)
            if len(badtests):
                raise ValueError('Test(s) <%s> requested but not available in Autotest' % (",".join(badtests)))

            if scratchNeeded:
                hw = self.kind_of_hardware()

                # We require a spare drive for xfstesting. We make an assumption about what that
                # spare drive is. Look at the device the root device is and assume that the scratch
                # drive is the 'b' device with that same device root.
                #
                testDependencies['xfstests']['atargs']['UBUNTU_SCRATCH_DEVICE'] = '/dev/%sb' % self.root_dev()

            # write the setup script
            with open('./presetup', 'w') as f:
                f.write("sudo apt-get update\n")
                for atname in autotestToRun:
                    print("Dependencies: %s" % atname)
                    if atname in testDependencies:
                        # handle package dependencies
                        tdeps = testDependencies[atname]
                        if 'packages' in tdeps:
                            f.write('sudo apt-get install -y ')
                            print("    %s" % tdeps['packages']['common'])
                            for pkg in tdeps['packages']['common']:
                                f.write('%s ' % pkg)
                            if self.releaseName in tdeps['packages']:
                                for pkg in tdeps['packages'][self.releaseName]:
                                    f.write('%s ' % pkg)
                            f.write('\n')
                        # Run any test-specific command lines
                        if 'command' in tdeps:
                            f.write('%s ' % tdeps['command'])
                            f.write('\n')

                # Write out environment variables
                for key, value in envVars.iteritems():
                    print("Writing environment variable %s = %s" % (key, value))
                    f.write('export %s=%s\n' % (key, value))

            #
            # Figure out which control files to use for each test
            # We look for control.ubuntu.<series>, then control.ubuntu, then control
            ctrlFiles = {}
            for testname in autotestToRun:
                print("Determining correct control file for test: %s" % testname)
                existingControlFiles = listdir('autotest/client/tests/%s/' % testname)
                print("    Selecting from these available control files:")
                for fn in existingControlFiles:
                    print("      %s" % fn)
                if ("control.ubuntu.%s" % self.releaseName) in existingControlFiles:
                    controlFile = "control.ubuntu.%s" % self.releaseName
                elif "control.ubuntu" in existingControlFiles:
                    controlFile = "control.ubuntu"
                else:
                    controlFile = "control"
                print("    Selected control file: %s" % controlFile)
                ctrlFiles[testname] = controlFile

            # Now write out the test script
            with open('./run-tests', 'w') as f:
                f.write("""
#!/bin/sh

# run-tests
#
# This shell script runs the autotest testcase that is specified on the
# command line. It gethers the autotest results and gets them back to
# the jenkins server.
#

set +x

if [ -z "$JENKINS_SERVER" ]; then
    JENKINS_SERVER=kernel-jenkins
fi
CONTROL_FILE=control
AUTOTEST_TEST=_bogus_

echo ===============
uname -a
echo ===============
set -x

# Now run the tests

""")
                for testname in autotestToRun:
                    # Run the test
                    f.write('sudo autotest/client/autotest')
                    try:
                        args = testDependencies[testname]['atargs']
                        print "args =", args
                        f.write(' --args="')
                        for key, value in args.iteritems():
                            f.write('%s=%s ' % (key, value))
                        f.write('"')

                    except KeyError as e:
                        print('\n*** INFO ***\n    No dependencies found for test "%s".\n' % (e.message))
                        pass
                    f.write(' autotest/client/tests/%s/%s\n' % (testname, ctrlFiles[testname]))
                    f.write('export TEST_NAME=%s\n' % testname)
                    # And postprocess the results
                    f.write('. $CDIR/job-postprocessing\n')

        except Exit:
            pass

        exit(retval)

if __name__ == '__main__':
    app = ExpandTests()
    app.main()

# vi:set ts=4 sw=4 expandtab:

