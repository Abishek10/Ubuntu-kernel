From: Steven Rostedt <srostedt@redhat.com>

With the added boost of SCHED_OTHER, the balancing load starts to stain
latencies. Bring it back down again.

NOTE: This is a workaround, we need to fix this because this work around
will once again hurt SCHED_OTHER performance.

Signed-off-by: Steven Rostedt <srostedt@redhat.com>
---
 kernel/sched.c |    4 ++++
 1 file changed, 4 insertions(+)

Index: linux-2.6.24.7-rt21/kernel/sched.c
===================================================================
--- linux-2.6.24.7-rt21.orig/kernel/sched.c	2008-10-08 22:24:50.000000000 -0400
+++ linux-2.6.24.7-rt21/kernel/sched.c	2008-10-08 22:24:53.000000000 -0400
@@ -602,7 +602,11 @@ const_debug unsigned int sysctl_sched_fe
  * Number of tasks to iterate in a single balance run.
  * Limited because this is done with IRQs disabled.
  */
+#ifdef CONFIG_PREEMPT_RT
+const_debug unsigned int sysctl_sched_nr_migrate = 8;
+#else
 const_debug unsigned int sysctl_sched_nr_migrate = 32;
+#endif
 
 /*
  * For kernel-internal use: high-speed (but slightly incorrect) per-cpu
