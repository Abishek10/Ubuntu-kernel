From: Steven Rostedt <srostedt@redhat.com>

With the added boost of SCHED_OTHER, the balancing load starts to stain
latencies. Bring it back down again.

NOTE: This is a workaround, we need to fix this because this work around
will once again hurt SCHED_OTHER performance.

Signed-off-by: Steven Rostedt <srostedt@redhat.com>
---
 kernel/sched.c |    4 ++++
 1 file changed, 4 insertions(+)

Index: linux-2.6.24.7-rt27/kernel/sched.c
===================================================================
--- linux-2.6.24.7-rt27.orig/kernel/sched.c	2009-02-08 00:04:22.000000000 -0500
+++ linux-2.6.24.7-rt27/kernel/sched.c	2009-02-08 00:04:27.000000000 -0500
@@ -602,7 +602,11 @@ const_debug unsigned int sysctl_sched_fe
  * Number of tasks to iterate in a single balance run.
  * Limited because this is done with IRQs disabled.
  */
+#ifdef CONFIG_PREEMPT_RT
+const_debug unsigned int sysctl_sched_nr_migrate = 8;
+#else
 const_debug unsigned int sysctl_sched_nr_migrate = 32;
+#endif
 
 /*
  * For kernel-internal use: high-speed (but slightly incorrect) per-cpu
