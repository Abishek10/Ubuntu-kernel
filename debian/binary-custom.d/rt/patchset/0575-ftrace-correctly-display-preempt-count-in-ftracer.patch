From: Carsten Emde <Carsten.Emde@osadl.org>
Subject: ftrace:  display real preempt_count in ftracer
Date: Wed, 28 Jan 2009 14:39:51 +0100

Ftrace determined the preempt_count after preemption was disabled
instead of the original preemption count.

Signed-off-by: Carsten Emde <C.Emde@osadl.org>
Signed-off-by: Thomas Gleixner <tglx@linutronix.de>

---
 kernel/trace/trace.c              |   48 +++++++++++++++++++-------------------
 kernel/trace/trace.h              |    3 +-
 kernel/trace/trace_hist.c         |    2 -
 kernel/trace/trace_irqsoff.c      |   13 ++++++----
 kernel/trace/trace_sched_wakeup.c |    9 ++++---
 5 files changed, 41 insertions(+), 34 deletions(-)

Index: linux-2.6.24.7-rt27/kernel/trace/trace.c
===================================================================
--- linux-2.6.24.7-rt27.orig/kernel/trace/trace.c	2009-02-08 00:05:21.000000000 -0500
+++ linux-2.6.24.7-rt27/kernel/trace/trace.c	2009-02-08 00:05:24.000000000 -0500
@@ -808,12 +808,10 @@ tracing_get_trace_entry(struct trace_arr
 }
 
 static inline void
-tracing_generic_entry_update(struct trace_entry *entry, unsigned long flags)
+tracing_generic_entry_update(struct trace_entry *entry, unsigned long flags,
+			     unsigned long pc)
 {
 	struct task_struct *tsk = current;
-	unsigned long pc;
-
-	pc = preempt_count();
 
 	entry->preempt_count	= pc & 0xff;
 	entry->pid		= (tsk) ? tsk->pid : 0;
@@ -826,7 +824,8 @@ tracing_generic_entry_update(struct trac
 
 void
 trace_function(struct trace_array *tr, struct trace_array_cpu *data,
-	       unsigned long ip, unsigned long parent_ip, unsigned long flags)
+	       unsigned long ip, unsigned long parent_ip, unsigned long flags,
+	       unsigned long pc)
 {
 	struct trace_entry *entry;
 	unsigned long irq_flags;
@@ -834,7 +833,7 @@ trace_function(struct trace_array *tr, s
 	raw_local_irq_save(irq_flags);
 	__raw_spin_lock(&data->lock);
 	entry			= tracing_get_trace_entry(tr, data);
-	tracing_generic_entry_update(entry, flags);
+	tracing_generic_entry_update(entry, flags, pc);
 	entry->type		= TRACE_FN;
 	entry->fn.ip		= ip;
 	entry->fn.parent_ip	= parent_ip;
@@ -847,7 +846,8 @@ ftrace(struct trace_array *tr, struct tr
        unsigned long ip, unsigned long parent_ip, unsigned long flags)
 {
 	if (likely(!atomic_read(&data->disabled)))
-		trace_function(tr, data, ip, parent_ip, flags);
+		trace_function(tr, data, ip, parent_ip, flags,
+			       preempt_count());
 }
 
 #ifdef CONFIG_MMIOTRACE
@@ -861,7 +861,7 @@ void __trace_mmiotrace_rw(struct trace_a
 	__raw_spin_lock(&data->lock);
 
 	entry			= tracing_get_trace_entry(tr, data);
-	tracing_generic_entry_update(entry, 0);
+	tracing_generic_entry_update(entry, 0, preempt_count());
 	entry->type		= TRACE_MMIO_RW;
 	entry->mmiorw		= *rw;
 
@@ -881,7 +881,7 @@ void __trace_mmiotrace_map(struct trace_
 	__raw_spin_lock(&data->lock);
 
 	entry			= tracing_get_trace_entry(tr, data);
-	tracing_generic_entry_update(entry, 0);
+	tracing_generic_entry_update(entry, 0, preempt_count());
 	entry->type		= TRACE_MMIO_MAP;
 	entry->mmiomap		= *map;
 
@@ -904,7 +904,7 @@ void __trace_stack(struct trace_array *t
 		return;
 
 	entry			= tracing_get_trace_entry(tr, data);
-	tracing_generic_entry_update(entry, flags);
+	tracing_generic_entry_update(entry, flags, preempt_count());
 	entry->type		= TRACE_STACK;
 
 	memset(&entry->stack, 0, sizeof(entry->stack));
@@ -929,7 +929,7 @@ __trace_special(void *__tr, void *__data
 	raw_local_irq_save(irq_flags);
 	__raw_spin_lock(&data->lock);
 	entry			= tracing_get_trace_entry(tr, data);
-	tracing_generic_entry_update(entry, 0);
+	tracing_generic_entry_update(entry, 0, preempt_count());
 	entry->type		= TRACE_SPECIAL;
 	entry->special.arg1	= arg1;
 	entry->special.arg2	= arg2;
@@ -954,7 +954,7 @@ tracing_sched_switch_trace(struct trace_
 	raw_local_irq_save(irq_flags);
 	__raw_spin_lock(&data->lock);
 	entry			= tracing_get_trace_entry(tr, data);
-	tracing_generic_entry_update(entry, flags);
+	tracing_generic_entry_update(entry, flags, preempt_count());
 	entry->type		= TRACE_CTX;
 	entry->ctx.prev_pid	= prev->pid;
 	entry->ctx.prev_prio	= prev->prio;
@@ -980,7 +980,7 @@ tracing_sched_wakeup_trace(struct trace_
 	raw_local_irq_save(irq_flags);
 	__raw_spin_lock(&data->lock);
 	entry			= tracing_get_trace_entry(tr, data);
-	tracing_generic_entry_update(entry, flags);
+	tracing_generic_entry_update(entry, flags, preempt_count());
 	entry->type		= TRACE_WAKE;
 	entry->ctx.prev_pid	= curr->pid;
 	entry->ctx.prev_prio	= curr->prio;
@@ -1029,7 +1029,7 @@ void tracing_event_irq(struct trace_arra
 	struct trace_entry *entry;
 
 	entry = tracing_get_trace_entry(tr, data);
-	tracing_generic_entry_update(entry, flags);
+	tracing_generic_entry_update(entry, flags, preempt_count());
 	entry->type		= TRACE_IRQ;
 	entry->irq.ip		= ip;
 	entry->irq.irq		= irq;
@@ -1048,7 +1048,7 @@ void tracing_event_fault(struct trace_ar
 	struct trace_entry *entry;
 
 	entry = tracing_get_trace_entry(tr, data);
-	tracing_generic_entry_update(entry, flags);
+	tracing_generic_entry_update(entry, flags, preempt_count());
 	entry->type		= TRACE_FAULT;
 	entry->fault.ip		= ip;
 	entry->fault.ret_ip	= retip;
@@ -1065,7 +1065,7 @@ void tracing_event_timer_set(struct trac
 	struct trace_entry *entry;
 
 	entry = tracing_get_trace_entry(tr, data);
-	tracing_generic_entry_update(entry, flags);
+	tracing_generic_entry_update(entry, flags, preempt_count());
 	entry->type		= TRACE_TIMER_SET;
 	entry->timer.ip		= ip;
 	entry->timer.expire	= *expires;
@@ -1081,7 +1081,7 @@ void tracing_event_program_event(struct 
 	struct trace_entry *entry;
 
 	entry = tracing_get_trace_entry(tr, data);
-	tracing_generic_entry_update(entry, flags);
+	tracing_generic_entry_update(entry, flags, preempt_count());
 	entry->type		= TRACE_PROGRAM_EVENT;
 	entry->program.ip	= ip;
 	entry->program.expire	= *expires;
@@ -1097,7 +1097,7 @@ void tracing_event_timer_triggered(struc
 	struct trace_entry *entry;
 
 	entry = tracing_get_trace_entry(tr, data);
-	tracing_generic_entry_update(entry, flags);
+	tracing_generic_entry_update(entry, flags, preempt_count());
 	entry->type		= TRACE_TIMER_TRIG;
 	entry->timer.ip		= ip;
 	entry->timer.expire	= *expired;
@@ -1113,7 +1113,7 @@ void tracing_event_timestamp(struct trac
 	struct trace_entry *entry;
 
 	entry = tracing_get_trace_entry(tr, data);
-	tracing_generic_entry_update(entry, flags);
+	tracing_generic_entry_update(entry, flags, preempt_count());
 	entry->type		= TRACE_TIMESTAMP;
 	entry->timestamp.ip		= ip;
 	entry->timestamp.now		= *now;
@@ -1129,7 +1129,7 @@ void tracing_event_task_activate(struct 
 	struct trace_entry *entry;
 
 	entry = tracing_get_trace_entry(tr, data);
-	tracing_generic_entry_update(entry, flags);
+	tracing_generic_entry_update(entry, flags, preempt_count());
 	entry->type		= TRACE_TASK_ACT;
 	entry->task.ip		= ip;
 	entry->task.pid		= p->pid;
@@ -1147,7 +1147,7 @@ void tracing_event_task_deactivate(struc
 	struct trace_entry *entry;
 
 	entry = tracing_get_trace_entry(tr, data);
-	tracing_generic_entry_update(entry, flags);
+	tracing_generic_entry_update(entry, flags, preempt_count());
 	entry->type		= TRACE_TASK_DEACT;
 	entry->task.ip		= ip;
 	entry->task.pid		= p->pid;
@@ -1167,7 +1167,7 @@ void tracing_event_syscall(struct trace_
 	struct trace_entry *entry;
 
 	entry = tracing_get_trace_entry(tr, data);
-	tracing_generic_entry_update(entry, flags);
+	tracing_generic_entry_update(entry, flags, preempt_count());
 	entry->type			= TRACE_SYSCALL;
 	entry->syscall.ip		= ip;
 	entry->syscall.nr		= nr;
@@ -1185,7 +1185,7 @@ void tracing_event_sysret(struct trace_a
 	struct trace_entry *entry;
 
 	entry = tracing_get_trace_entry(tr, data);
-	tracing_generic_entry_update(entry, flags);
+	tracing_generic_entry_update(entry, flags, preempt_count());
 	entry->type			= TRACE_SYSRET;
 	entry->sysret.ip		= ip;
 	entry->sysret.ret		= ret;
@@ -1210,7 +1210,7 @@ function_trace_call(unsigned long ip, un
 	disabled = atomic_inc_return(&data->disabled);
 
 	if (likely(disabled == 1))
-		trace_function(tr, data, ip, parent_ip, flags);
+		trace_function(tr, data, ip, parent_ip, flags, preempt_count());
 
 	atomic_dec(&data->disabled);
 	local_irq_restore(flags);
Index: linux-2.6.24.7-rt27/kernel/trace/trace.h
===================================================================
--- linux-2.6.24.7-rt27.orig/kernel/trace/trace.h	2009-02-08 00:05:05.000000000 -0500
+++ linux-2.6.24.7-rt27/kernel/trace/trace.h	2009-02-08 00:05:24.000000000 -0500
@@ -302,7 +302,8 @@ void trace_function(struct trace_array *
 		    struct trace_array_cpu *data,
 		    unsigned long ip,
 		    unsigned long parent_ip,
-		    unsigned long flags);
+		    unsigned long flags,
+		    unsigned long pc);
 void tracing_event_irq(struct trace_array *tr,
 		       struct trace_array_cpu *data,
 		       unsigned long flags,
Index: linux-2.6.24.7-rt27/kernel/trace/trace_hist.c
===================================================================
--- linux-2.6.24.7-rt27.orig/kernel/trace/trace_hist.c	2009-02-08 00:05:23.000000000 -0500
+++ linux-2.6.24.7-rt27/kernel/trace/trace_hist.c	2009-02-08 00:05:24.000000000 -0500
@@ -354,7 +354,7 @@ notrace void tracing_hist_preempt_stop(i
 	cpu = raw_smp_processor_id();
 
 #ifdef CONFIG_INTERRUPT_OFF_HIST
-	if (irqs_on  &&
+	if (irqs_on &&
 	    per_cpu(hist_irqsoff_tracing, cpu)) {
 		stop = ftrace_now(cpu);
 		stop_set++;
Index: linux-2.6.24.7-rt27/kernel/trace/trace_irqsoff.c
===================================================================
--- linux-2.6.24.7-rt27.orig/kernel/trace/trace_irqsoff.c	2009-02-08 00:05:19.000000000 -0500
+++ linux-2.6.24.7-rt27/kernel/trace/trace_irqsoff.c	2009-02-08 00:05:24.000000000 -0500
@@ -99,7 +99,8 @@ irqsoff_tracer_call(unsigned long ip, un
 	disabled = atomic_inc_return(&data->disabled);
 
 	if (likely(disabled == 1))
-		trace_function(tr, data, ip, parent_ip, flags);
+		trace_function(tr, data, ip, parent_ip, flags,
+			       preempt_count());
 
 	atomic_dec(&data->disabled);
 }
@@ -154,7 +155,8 @@ check_critical_timing(struct trace_array
 	if (!report_latency(delta))
 		goto out_unlock;
 
-	trace_function(tr, data, CALLER_ADDR0, parent_ip, flags);
+	trace_function(tr, data, CALLER_ADDR0, parent_ip, flags,
+		       preempt_count());
 
 	latency = nsecs_to_usecs(delta);
 
@@ -178,7 +180,8 @@ out:
 	data->critical_sequence = max_sequence;
 	data->preempt_timestamp = ftrace_now(cpu);
 	tracing_reset(data);
-	trace_function(tr, data, CALLER_ADDR0, parent_ip, flags);
+	trace_function(tr, data, CALLER_ADDR0, parent_ip, flags,
+		       preempt_count());
 }
 
 static inline void
@@ -211,7 +214,7 @@ start_critical_timing(unsigned long ip, 
 
 	local_save_flags(flags);
 
-	trace_function(tr, data, ip, parent_ip, flags);
+	trace_function(tr, data, ip, parent_ip, flags, preempt_count());
 
 	per_cpu(tracing_cpu, cpu) = 1;
 
@@ -245,7 +248,7 @@ stop_critical_timing(unsigned long ip, u
 	atomic_inc(&data->disabled);
 
 	local_save_flags(flags);
-	trace_function(tr, data, ip, parent_ip, flags);
+	trace_function(tr, data, ip, parent_ip, flags, preempt_count());
 	check_critical_timing(tr, data, parent_ip ? : ip, cpu);
 	data->critical_start = 0;
 	atomic_dec(&data->disabled);
Index: linux-2.6.24.7-rt27/kernel/trace/trace_sched_wakeup.c
===================================================================
--- linux-2.6.24.7-rt27.orig/kernel/trace/trace_sched_wakeup.c	2009-02-08 00:04:48.000000000 -0500
+++ linux-2.6.24.7-rt27/kernel/trace/trace_sched_wakeup.c	2009-02-08 00:05:24.000000000 -0500
@@ -44,10 +44,12 @@ wakeup_tracer_call(unsigned long ip, uns
 	long disabled;
 	int resched;
 	int cpu;
+	unsigned long pc;
 
 	if (likely(!wakeup_task) || !ftrace_enabled)
 		return;
 
+	pc = preempt_count();
 	resched = need_resched();
 	preempt_disable_notrace();
 
@@ -70,7 +72,7 @@ wakeup_tracer_call(unsigned long ip, uns
 	if (task_cpu(wakeup_task) != cpu)
 		goto unlock;
 
-	trace_function(tr, data, ip, parent_ip, flags);
+	trace_function(tr, data, ip, parent_ip, flags, pc);
 
  unlock:
 	__raw_spin_unlock(&wakeup_lock);
@@ -155,7 +157,8 @@ wakeup_sched_switch(void *private, void 
 	if (unlikely(!tracer_enabled || next != wakeup_task))
 		goto out_unlock;
 
-	trace_function(tr, data, CALLER_ADDR1, CALLER_ADDR2, flags);
+	trace_function(tr, data, CALLER_ADDR1, CALLER_ADDR2, flags,
+		       preempt_count());
 
 	/*
 	 * usecs conversion is slow so we try to delay the conversion
@@ -276,7 +279,7 @@ wakeup_check_start(struct trace_array *t
 
 	tr->data[wakeup_cpu]->preempt_timestamp = ftrace_now(cpu);
 	trace_function(tr, tr->data[wakeup_cpu],
-		       CALLER_ADDR1, CALLER_ADDR2, flags);
+		       CALLER_ADDR1, CALLER_ADDR2, flags, preempt_count());
 
 out_locked:
 	__raw_spin_unlock(&wakeup_lock);
