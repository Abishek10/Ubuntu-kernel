Subject: sched: lock-break the load balance path
From: Peter Zijlstra <a.p.zijlstra@chello.nl>

move_tasks() can do a lot of work, and it holds two runqueue locks and has
IRQs disabled. We already introduced sysctl_sched_nr_migrate to limit
the number of task iterations it can do.

This patch takes it one step further and drops the locks once we break out
of the iteration due to sysctl_sched_nr_migrate and re-enables IRQs. Then it
re-acquires everything and continues.

Dropping the locks is safe because:
 - load_balance() doesn't rely on it
 - load_balance_newidle() uses double_lock_balance() which
   can already drop the locks.

Enabling IRQs should be safe since we already dropped all locks.

We add the LB_COMPLETE state to detect the truncated iteration due to
sysctl_sched_nr_migrate.

For now we must break out of the restart when load_moved is 0, because each
iteration will test the same tasks - hence we can live-lock here.

Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
---
 kernel/sched.c       |   42 +++++++++++++++++++++++++++++++++++++-----
 kernel/sched_debug.c |   28 ++++++++++++++++++++++------
 2 files changed, 59 insertions(+), 11 deletions(-)

Index: linux-2.6.24.7-rt21/kernel/sched.c
===================================================================
--- linux-2.6.24.7-rt21.orig/kernel/sched.c	2008-10-08 22:24:54.000000000 -0400
+++ linux-2.6.24.7-rt21/kernel/sched.c	2008-10-08 22:24:54.000000000 -0400
@@ -461,6 +461,8 @@ struct rq {
 	unsigned long rto_wakeup;
 	unsigned long rto_pulled;
 	unsigned long rto_pushed;
+
+	unsigned long lb_breaks;
 #endif
 	struct lock_class_key rq_lock_key;
 };
@@ -587,6 +589,7 @@ enum {
 	SCHED_FEAT_START_DEBIT		= 4,
 	SCHED_FEAT_TREE_AVG		= 8,
 	SCHED_FEAT_APPROX_AVG		= 16,
+	SCHED_FEAT_LB_BREAK		= 32,
 };
 
 const_debug unsigned int sysctl_sched_features =
@@ -594,7 +597,8 @@ const_debug unsigned int sysctl_sched_fe
 		SCHED_FEAT_WAKEUP_PREEMPT	* 1 |
 		SCHED_FEAT_START_DEBIT		* 1 |
 		SCHED_FEAT_TREE_AVG		* 0 |
-		SCHED_FEAT_APPROX_AVG		* 0;
+		SCHED_FEAT_APPROX_AVG		* 0 |
+		SCHED_FEAT_LB_BREAK		* 1;
 
 #define sched_feat(x) (sysctl_sched_features & SCHED_FEAT_##x)
 
@@ -2270,6 +2274,7 @@ static void update_cpu_load(struct rq *t
 #ifdef CONFIG_SMP
 
 #define LB_ALL_PINNED	0x01
+#define LB_COMPLETE	0x02
 
 /*
  * double_rq_lock - safely lock two runqueues
@@ -2476,8 +2481,13 @@ balance_tasks(struct rq *this_rq, int th
 	if (p)
 		pinned = 1;
 next:
-	if (!p || loops++ > sysctl_sched_nr_migrate)
+	if (!p)
+	       goto out;
+
+	if (loops++ > sysctl_sched_nr_migrate) {
+		*lb_flags &= ~LB_COMPLETE;
 		goto out;
+	}
 	/*
 	 * To help distribute high priority tasks across CPUs we don't
 	 * skip a task if it will be the highest priority task (i.e. smallest
@@ -2535,11 +2545,30 @@ static int move_tasks(struct rq *this_rq
 	int this_best_prio = this_rq->curr->prio;
 
 	do {
-		total_load_moved +=
-			class->load_balance(this_rq, this_cpu, busiest,
+		unsigned long load_moved;
+
+		*lb_flags |= LB_COMPLETE;
+
+		load_moved = class->load_balance(this_rq, this_cpu, busiest,
 				max_load_move - total_load_moved,
 				sd, idle, lb_flags, &this_best_prio);
-		class = class->next;
+
+		total_load_moved += load_moved;
+
+		if (!load_moved || *lb_flags & LB_COMPLETE) {
+			class = class->next;
+		} else if (sched_feat(LB_BREAK)) {
+			schedstat_inc(this_rq, lb_breaks);
+
+			double_rq_unlock(this_rq, busiest);
+			local_irq_enable();
+
+			if (!in_atomic())
+				cond_resched();
+
+			local_irq_disable();
+			double_rq_lock(this_rq, busiest);
+		}
 	} while (class && max_load_move > total_load_moved);
 
 	return total_load_moved > 0;
@@ -2983,6 +3012,9 @@ redo:
 
 	ld_moved = 0;
 	if (busiest->nr_running > 1) {
+
+		WARN_ON(irqs_disabled());
+
 		/*
 		 * Attempt to move tasks. If find_busiest_group has found
 		 * an imbalance but busiest->nr_running <= 1, the group is
Index: linux-2.6.24.7-rt21/kernel/sched_debug.c
===================================================================
--- linux-2.6.24.7-rt21.orig/kernel/sched_debug.c	2008-10-08 22:24:25.000000000 -0400
+++ linux-2.6.24.7-rt21/kernel/sched_debug.c	2008-10-08 22:24:54.000000000 -0400
@@ -186,17 +186,33 @@ static void print_cpu(struct seq_file *m
 	P(cpu_load[2]);
 	P(cpu_load[3]);
 	P(cpu_load[4]);
-#ifdef CONFIG_PREEMPT_RT
-	/* Print rt related rq stats */
-	P(rt.rt_nr_running);
-	P(rt.rt_nr_uninterruptible);
-# ifdef CONFIG_SCHEDSTATS
+#ifdef CONFIG_SCHEDSTATS
+	P(yld_exp_empty);
+	P(yld_act_empty);
+	P(yld_both_empty);
+	P(yld_count);
+
+	P(sched_switch);
+	P(sched_count);
+	P(sched_goidle);
+
+	P(ttwu_count);
+	P(ttwu_local);
+
+	P(bkl_count);
+
 	P(rto_schedule);
 	P(rto_schedule_tail);
 	P(rto_wakeup);
 	P(rto_pulled);
 	P(rto_pushed);
-# endif
+
+	P(lb_breaks);
+#endif
+#ifdef CONFIG_PREEMPT_RT
+	/* Print rt related rq stats */
+	P(rt.rt_nr_running);
+	P(rt.rt_nr_uninterruptible);
 #endif
 
 #undef P
