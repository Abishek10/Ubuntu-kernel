x86: disable spinlock preempt feature since we have ticketlocks

From: Gregory Haskins <ghaskins@novell.com>

The spinlock preempt feature utilizes spin_trylock() to implement
preemptible waiters.  However, doing so circumvents the benefit of
using a FIFO/ticket lock, so we disable the feature when ticketlocks
are enabled.

Signed-off-by: Gregory Haskins <ghaskins@novell.com>
CC: Nick Piggin <npiggin@suse.de>
---

 kernel/spinlock.c |    5 ++++-
 1 file changed, 4 insertions(+), 1 deletion(-)

Index: linux-2.6.24.7-rt27/kernel/spinlock.c
===================================================================
--- linux-2.6.24.7-rt27.orig/kernel/spinlock.c	2009-02-08 00:04:15.000000000 -0500
+++ linux-2.6.24.7-rt27/kernel/spinlock.c	2009-02-08 00:04:37.000000000 -0500
@@ -115,9 +115,12 @@ EXPORT_SYMBOL(__write_trylock_irqsave);
  * If lockdep is enabled then we use the non-preemption spin-ops
  * even on CONFIG_PREEMPT, because lockdep assumes that interrupts are
  * not re-enabled during lock-acquire (which the preempt-spin-ops do):
+ *
+ * We also disable them on x86 because we now have ticket/fifo locks,
+ * which are defeated using a preemptible spinlock
  */
 #if !defined(CONFIG_PREEMPT) || !defined(CONFIG_SMP) || \
-	defined(CONFIG_DEBUG_LOCK_ALLOC)
+	defined(CONFIG_DEBUG_LOCK_ALLOC) || defined(CONFIG_X86)
 
 void __lockfunc __read_lock(raw_rwlock_t *lock)
 {
