Subject: sched: even weaker newidle balancing
From: Peter Zijlstra <a.p.zijlstra@chello.nl>

On each round see if any of the classes became runnable - if so, stop
balancing and run the thing.

Signed-off-by: Peter Zijlstra <a.p.zijlstra@chello.nl>
---
 include/linux/sched.h   |    2 ++
 kernel/sched.c          |   28 ++++++++++++++++++++--------
 kernel/sched_fair.c     |    7 +++++++
 kernel/sched_idletask.c |    7 +++++++
 kernel/sched_rt.c       |    6 ++++++
 5 files changed, 42 insertions(+), 8 deletions(-)

Index: linux-2.6.24.7-rt21/include/linux/sched.h
===================================================================
--- linux-2.6.24.7-rt21.orig/include/linux/sched.h	2008-10-08 22:24:52.000000000 -0400
+++ linux-2.6.24.7-rt21/include/linux/sched.h	2008-10-08 22:24:55.000000000 -0400
@@ -922,6 +922,8 @@ struct sched_class {
 	void (*pre_schedule) (struct rq *this_rq, struct task_struct *task);
 	void (*post_schedule) (struct rq *this_rq);
 	void (*task_wake_up) (struct rq *this_rq, struct task_struct *task);
+
+	int (*is_runnable) (struct rq *this_rq);
 #endif
 
 	void (*set_curr_task) (struct rq *rq);
Index: linux-2.6.24.7-rt21/kernel/sched.c
===================================================================
--- linux-2.6.24.7-rt21.orig/kernel/sched.c	2008-10-08 22:24:55.000000000 -0400
+++ linux-2.6.24.7-rt21/kernel/sched.c	2008-10-08 22:24:55.000000000 -0400
@@ -2533,6 +2533,21 @@ out:
 	return max_load_move - rem_load_move;
 }
 
+static int is_runnable(struct rq *this_rq, const struct sched_class *target_class)
+{
+	const struct sched_class *class = sched_class_highest;
+
+	for (; class; class = class->next) {
+		if (class->is_runnable(this_rq))
+			return 1;
+
+		if (class == target_class)
+			break;
+	}
+
+	return 0;
+}
+
 /*
  * move_tasks tries to move up to max_load_move weighted load from busiest to
  * this_rq, as part of a balancing operation within domain "sd".
@@ -2552,15 +2567,15 @@ static int move_tasks(struct rq *this_rq
 	*lb_flags |= LB_START;
 
 	do {
-		unsigned long load_moved;
-
 		*lb_flags |= LB_COMPLETE;
 
-		load_moved = class->load_balance(this_rq, this_cpu, busiest,
-				max_load_move - total_load_moved,
+		total_load_moved += class->load_balance(this_rq, this_cpu,
+				busiest, max_load_move - total_load_moved,
 				sd, idle, lb_flags, &this_best_prio);
 
-		total_load_moved += load_moved;
+		if (idle == CPU_NEWLY_IDLE &&
+				is_runnable(this_rq, class))
+			return 1;
 
 		if (*lb_flags & LB_COMPLETE) {
 			class = class->next;
@@ -2569,9 +2584,6 @@ static int move_tasks(struct rq *this_rq
 			*lb_flags &= ~LB_START;
 			schedstat_inc(this_rq, lb_breaks);
 
-			if (idle == CPU_NEWLY_IDLE && total_load_moved)
-				break;
-
 			double_rq_unlock(this_rq, busiest);
 			local_irq_enable();
 
Index: linux-2.6.24.7-rt21/kernel/sched_fair.c
===================================================================
--- linux-2.6.24.7-rt21.orig/kernel/sched_fair.c	2008-10-08 22:24:54.000000000 -0400
+++ linux-2.6.24.7-rt21/kernel/sched_fair.c	2008-10-08 22:24:55.000000000 -0400
@@ -1188,6 +1188,12 @@ move_one_task_fair(struct rq *this_rq, i
 
 	return 0;
 }
+
+static int
+is_runnable_fair(struct rq *this_rq)
+{
+	return !!this_rq->cfs.nr_running;
+}
 #endif
 
 /*
@@ -1307,6 +1313,7 @@ static const struct sched_class fair_sch
 #ifdef CONFIG_SMP
 	.load_balance		= load_balance_fair,
 	.move_one_task		= move_one_task_fair,
+	.is_runnable		= is_runnable_fair,
 #endif
 
 	.set_curr_task          = set_curr_task_fair,
Index: linux-2.6.24.7-rt21/kernel/sched_idletask.c
===================================================================
--- linux-2.6.24.7-rt21.orig/kernel/sched_idletask.c	2008-10-08 22:24:54.000000000 -0400
+++ linux-2.6.24.7-rt21/kernel/sched_idletask.c	2008-10-08 22:24:55.000000000 -0400
@@ -59,6 +59,12 @@ move_one_task_idle(struct rq *this_rq, i
 {
 	return 0;
 }
+
+static int
+is_runnable_idle(struct rq *this_rq)
+{
+	return 1;
+}
 #endif
 
 static void task_tick_idle(struct rq *rq, struct task_struct *curr)
@@ -117,6 +123,7 @@ const struct sched_class idle_sched_clas
 #ifdef CONFIG_SMP
 	.load_balance		= load_balance_idle,
 	.move_one_task		= move_one_task_idle,
+	.is_runnable		= is_runnable_idle,
 #endif
 
 	.set_curr_task          = set_curr_task_idle,
Index: linux-2.6.24.7-rt21/kernel/sched_rt.c
===================================================================
--- linux-2.6.24.7-rt21.orig/kernel/sched_rt.c	2008-10-08 22:24:54.000000000 -0400
+++ linux-2.6.24.7-rt21/kernel/sched_rt.c	2008-10-08 22:24:55.000000000 -0400
@@ -792,6 +792,11 @@ static void switched_from_rt(struct rq *
 	if (!rq->rt.rt_nr_running)
 		pull_rt_task(rq);
 }
+
+static int is_runnable_rt(struct rq *rq)
+{
+	return !!rq->rt.rt_nr_running;
+}
 #endif /* CONFIG_SMP */
 
 /*
@@ -920,6 +925,7 @@ const struct sched_class rt_sched_class 
 	.post_schedule		= post_schedule_rt,
 	.task_wake_up		= task_wake_up_rt,
 	.switched_from		= switched_from_rt,
+	.is_runnable		= is_runnable_rt,
 #endif
 
 	.set_curr_task          = set_curr_task_rt,
